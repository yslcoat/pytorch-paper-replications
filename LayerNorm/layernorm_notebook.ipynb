{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0290003",
   "metadata": {},
   "source": [
    "An implementation of https://arxiv.org/pdf/1607.06450"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8e065",
   "metadata": {},
   "source": [
    "$$ \\mu^{l} = \\frac{1}{H} \\sum_{i=1}^{H} a^{l}_{i} \\qquad \\sigma^l=\\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(a_{i}^{l} - \\mu^l)Â²}$$\n",
    ", where $H$ denotes the number of hidden units in a layer, and $a_{i}^{l}$ is the actication of the $i$-th unit in layer $l$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation mean shape: torch.Size([32, 1])\n",
      "Activation variance shape: torch.Size([32, 1])\n",
      "x_hat shape: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_FEATURES = 128\n",
    "\n",
    "\n",
    "def layernorm(x: torch.Tensor, eps: float = 1e-5):\n",
    "    activation_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "    print(f\"Activation mean shape: {activation_mean.shape}\")\n",
    "    activation_variance = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "    print(f\"Activation variance shape: {activation_variance.shape}\")\n",
    "    x_hat = (x - activation_mean) / torch.sqrt(activation_variance + eps)\n",
    "    print(f\"x_hat shape: {x_hat.shape}\")\n",
    "\n",
    "    return x_hat\n",
    "\n",
    "\n",
    "x = torch.rand([BATCH_SIZE, N_FEATURES])\n",
    "normalized_x = layernorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc00be",
   "metadata": {},
   "source": [
    "Similar to batch normalization we add two learnable parameters, $\\gamma$ and $\\beta$ which we use to shift and scale the normalized tensor: $y^{(k)} = \\gamma^{(k)}\\widehat{x}^{(k)} + \\beta^{(k)} $. Since layernorm is stateless we don't need to calculate the running mean and variance as we do for batchnorm, and we don't require separate modes for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f938ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, n_features: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(self.n_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.n_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        activation_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        activation_var = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "        x_hat = (x - activation_mean) / torch.sqrt(activation_var + self.eps)\n",
    "        scaled_and_shifted = self.gamma * x_hat + self.beta\n",
    "\n",
    "        return scaled_and_shifted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_base_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
