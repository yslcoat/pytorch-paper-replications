{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215e1b0f",
   "metadata": {},
   "source": [
    "Implementation of SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER: https://arxiv.org/pdf/1701.06538"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b2a38",
   "metadata": {},
   "source": [
    "## Gating Network\n",
    "For each expert, the gate will output a score which will determine of the expert gets activated or not. If the output for any particular expert is 0 from the gate, $E_i (x)$ is not computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c50851",
   "metadata": {},
   "source": [
    "Softmax Gating: $G_\\sigma (x) = Softmax(x \\cdot W_g) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SoftmaxGating(nn.Module):\n",
    "    def __init__(self, input_emb_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(input_emb_dim, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.gate(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74289b12",
   "metadata": {},
   "source": [
    "Noisy Top-K Gating: $$ G(x) = Softmax(KeepTopK(H(x), k)) $$ $$H(x)_i = (x \\cdot W_g)_i + StandardNormal() \\cdot Softplus((x \\cdot W_{noise})_i) $$ $$ KeepTopK(v,k)_i = v_i \\text{   if    } v_i \\text{  is in the top   } k \\text{  elements of } v, -\\inf \\text{otherwise} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopKGating(nn.Module):\n",
    "    def __init__(self, input_emb_dim, num_experts, k):\n",
    "        super().__init__()\n",
    "        self.input_emb_dim = input_emb_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "\n",
    "        self.gate = nn.Linear(input_emb_dim, num_experts)\n",
    "        self.noise = nn.Linear(input_emb_dim, num_experts)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.softplus = nn.Softplus(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_output = self.gate(x)\n",
    "        noise_tensor = torch.randn(self.input_emb_dim, self.num_experts)\n",
    "        tuneable_noise = torch.matmul(self.standard_normal, self.softplus(self.noise(x)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
