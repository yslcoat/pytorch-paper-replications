{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215e1b0f",
   "metadata": {},
   "source": [
    "Implementation of SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER: https://arxiv.org/pdf/1701.06538"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b2a38",
   "metadata": {},
   "source": [
    "## Gating Network\n",
    "For each expert, the gate will output a score which will determine of the expert gets activated or not. If the output for any particular expert is 0 from the gate, $E_i (x)$ is not computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c50851",
   "metadata": {},
   "source": [
    "Softmax Gating: $G_\\sigma (x) = Softmax(x \\cdot W_g) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SoftmaxGating(nn.Module):\n",
    "    def __init__(self, input_emb_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(input_emb_dim, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.gate(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74289b12",
   "metadata": {},
   "source": [
    "Noisy Top-K Gating: $$ G(x) = Softmax(KeepTopK(H(x), k)) $$ $$H(x)_i = (x \\cdot W_g)_i + StandardNormal() \\cdot Softplus((x \\cdot W_{noise})_i) $$ $$ KeepTopK(v,k)_i = v_i \\text{   if    } v_i \\text{  is in the top   } k \\text{  elements of } v, -\\infty \\text{  otherwise} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopKGating(nn.Module):\n",
    "    def __init__(self, input_emb_dim, num_experts, k):\n",
    "        super().__init__()\n",
    "        self.input_emb_dim = input_emb_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "\n",
    "        self.gate = nn.Linear(input_emb_dim, num_experts)\n",
    "        self.noise = nn.Linear(input_emb_dim, num_experts)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_output = self.gate(x)\n",
    "\n",
    "        if self.training:\n",
    "            noise_std = self.softplus(self.noise(x))\n",
    "            noise_tensor = torch.randn_like(gate_output) * noise_std\n",
    "            noise_logits = gate_output + noise_tensor\n",
    "        else:\n",
    "            noise_logits = gate_output # Deterministic behavior during inference\n",
    "\n",
    "        top_k_logits, top_k_indices = torch.topk(noise_logits, self.k, dim=-1) # Gets topk logits and their indicies\n",
    "        full_logits = torch.full_like(noise_logits, float('-inf')) # creates a tensor with same shape as input tensor with fill values\n",
    "        full_logits.scatter_(-1, top_k_indices, top_k_logits) # replaces values in last dimension, in positions top_k_indices with values top_k_logits\n",
    "        gate_probs = self.softmax(full_logits)\n",
    "\n",
    "        return gate_probs, top_k_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6965b",
   "metadata": {},
   "source": [
    "Next we implement the MoE module: $$ y = \\sum_{i=1}^n G(x)_i E_i(x) $$, where $y$ is the output of the MoE module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, expert_hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, expert_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expert_hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, input_emb_dim, experts_output_dim, num_experts, k):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts_output_dim = experts_output_dim\n",
    "        self.k = k\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_emb_dim, experts_output_dim) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = NoisyTopKGating(input_emb_dim, num_experts, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_output, _ = self.gate(x)\n",
    "\n",
    "        final_output = torch.zeros(x.size(0), self.experts_output_dim, device=x.device)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_mask = gate_output[:, i] > 0\n",
    "            if expert_mask.any():\n",
    "                selection_indices = expert_mask.nonzero(as_tuple=True)[0] # Gets the indices of the experts\n",
    "                selected_input = x[selection_indices] # Gets input to the selected experts \n",
    "                \n",
    "                expert_output = expert(selected_input)\n",
    "                gate_weight = gate_output[selection_indices, i].unsqueeze(1) # Gets output of gate\n",
    "                \n",
    "                weighted_output = expert_output * gate_weight # Weights expert output with gate output\n",
    "\n",
    "                final_output.index_add_(0, selection_indices, weighted_output) # Accumulates the current expert's weighted output into the correct batch positions of the final tensor\n",
    "                \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acac583",
   "metadata": {},
   "source": [
    "### MoELayer implementation breakdown:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
