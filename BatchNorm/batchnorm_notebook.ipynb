{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3aa9bd3",
   "metadata": {},
   "source": [
    "An implementation of https://arxiv.org/pdf/1502.03167"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8618bc",
   "metadata": {},
   "source": [
    "$$\\widehat{x}^{(k)} = \\frac{x^{(k)} - \\text{E}[x^{(k)} ]}{\\sqrt{\\text{Var}[x^{(k)}]}}$$ , where k is the dimension of a d-dimensional input $$x = (x^{(1)} \\text{... } x^{(d)})$$, meaning we normalize for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be6f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerator tensor shape: torch.Size([32, 128])\n",
      "Normalized tensor shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_FEATURES = 128\n",
    "\n",
    "\n",
    "def normalize(x: torch.Tensor, eps: float = 1e-5):\n",
    "    # x.shape = batch_size, n_features\n",
    "    numerator = x - torch.mean(x, dim=0) # dim 0 since we wanna collapse the batch dimension (collapsing all rows into a single row)\n",
    "    print(f\"Numerator tensor shape: {numerator.shape}\")\n",
    "    denominator = torch.sqrt(torch.var(x, dim=0, unbiased=False) + eps)\n",
    "    normalized_tensor = numerator/denominator\n",
    "    print(f\"Normalized tensor shape: {normalized_tensor.shape}\")\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "input_tensor = torch.rand(BATCH_SIZE, NUM_FEATURES)\n",
    "normalized_tensor = normalize(input_tensor)\n",
    "normalized_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef573ac3",
   "metadata": {},
   "source": [
    "We then add two parameters, gamma and beta, which we use to scale and shift the normalized tensor: $$y^{(k)} = \\gamma^{(k)}\\widehat{x}^{(k)} + \\beta^{(k)} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c20c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = nn.Parameter(torch.ones(NUM_FEATURES))\n",
    "beta = nn.Parameter(torch.zeros(NUM_FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af48a0",
   "metadata": {},
   "source": [
    "We calculate the exponential moving average to estimate the mean and variance of the entire training set using the following formula: $$ EMA = (1 - \\alpha) \\cdot \\text{old} + \\alpha \\cdot \\text{new} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1782e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, n_features: int, eps: float = 1e-5, momentum: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(self.n_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.n_features))\n",
    "\n",
    "        # Used to calculate EMA instead of calculating mean and variance of the entire training set after training. \n",
    "        self.register_buffer('running_mean', torch.zeros(n_features))\n",
    "        self.register_buffer('running_var', torch.ones(n_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.training:\n",
    "            batch_mean = torch.mean(x, dim=0)\n",
    "            batch_var = torch.var(x, dim=0, unbiased=False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "            mean = batch_mean\n",
    "            var = batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        scaled_and_shifted = self.gamma * x_hat + self.beta\n",
    "\n",
    "        return scaled_and_shifted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_base_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
