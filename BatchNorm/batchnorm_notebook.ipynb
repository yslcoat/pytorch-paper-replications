{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3aa9bd3",
   "metadata": {},
   "source": [
    "An implementation of https://arxiv.org/pdf/1502.03167"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8618bc",
   "metadata": {},
   "source": [
    "$$\\widehat{x}^{(k)} = \\frac{x^{(k)} - \\text{E}[x^{(k)} ]}{\\sqrt{\\text{Var}[x^{(k)}]}}$$ , where k is the dimension of a d-dimensional input $$x = (x^{(1)} \\text{... } x^{(d)})$$, meaning we normalize for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be6f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerator tensor shape: torch.Size([32, 128])\n",
      "Normalized tensor shape: torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_FEATURES = 128\n",
    "\n",
    "\n",
    "def normalize(x: torch.Tensor, eps: float = 1e-5):\n",
    "    # x.shape = batch_size, n_features\n",
    "    numerator = x - torch.mean(x, dim=0) # dim 0 since we wanna collapse the batch dimension (collapsing all rows into a single row)\n",
    "    print(f\"Numerator tensor shape: {numerator.shape}\")\n",
    "    denominator = torch.sqrt(torch.var(x, dim=0, unbiased=False) + eps)\n",
    "    normalized_tensor = numerator/denominator\n",
    "    print(f\"Normalized tensor shape: {normalized_tensor.shape}\")\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "input_tensor = torch.rand(BATCH_SIZE, NUM_FEATURES)\n",
    "normalized_tensor = normalize(input_tensor)\n",
    "normalized_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef573ac3",
   "metadata": {},
   "source": [
    "We then add two parameters, gamma and beta, which we use to scale and shift the normalized tensor: $$y^{(k)} = \\gamma^{(k)}\\widehat{x}^{(k)} + \\beta^{(k)} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c20c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = nn.Parameter(torch.ones(NUM_FEATURES))\n",
    "beta = nn.Parameter(torch.zeros(NUM_FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af48a0",
   "metadata": {},
   "source": [
    "We calculate the exponential moving average to estimate the mean and variance of the entire training set using the following formula: $$ EMA = (1 - \\alpha) \\cdot \\text{old} + \\alpha \\cdot \\text{new} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1782e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, n_features: int, eps: float = 1e-5, momentum: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(self.n_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.n_features))\n",
    "\n",
    "        # Used to calculate EMA instead of calculating mean and variance of the entire training set after training. \n",
    "        self.register_buffer('running_mean', torch.zeros(n_features))\n",
    "        self.register_buffer('running_var', torch.ones(n_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x.shape: [B, N_FEATURES]\n",
    "        if self.training:\n",
    "            batch_mean = torch.mean(x, dim=0)\n",
    "            batch_var = torch.var(x, dim=0, unbiased=False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "            mean = batch_mean\n",
    "            var = batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        scaled_and_shifted = self.gamma * x_hat + self.beta\n",
    "\n",
    "        return scaled_and_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0137f6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6791,  0.9368, -0.8219,  ...,  1.1410,  0.3369, -0.0038],\n",
       "        [-1.4530, -1.5025, -0.4494,  ...,  0.0060, -1.4156,  0.1819],\n",
       "        [ 0.6202, -0.5847, -0.9721,  ...,  0.0415,  0.5601,  1.5636],\n",
       "        ...,\n",
       "        [ 1.3314, -0.3402, -1.1348,  ..., -0.2988,  1.1874, -1.0064],\n",
       "        [-1.1594,  1.2680, -0.9108,  ...,  0.2650, -1.0203, -1.4529],\n",
       "        [ 0.7662, -1.6140,  1.8213,  ...,  0.2648, -1.1049,  0.3148]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm = BatchNorm(NUM_FEATURES)\n",
    "\n",
    "normalized_tensor2 = batchnorm(input_tensor)\n",
    "normalized_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2029d5",
   "metadata": {},
   "source": [
    "batchnorm = BatchNorm(NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ecdb4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerator tensor shape: torch.Size([32, 3, 32, 32])\n",
      "Normalized tensor shape: torch.Size([32, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "\n",
    "\n",
    "def normalize2D(x: torch.Tensor, eps: float = 1e-5):\n",
    "    # x.shape = [B, C, H, W]\n",
    "    numerator = x - torch.mean(x, dim=(0, 2, 3), keepdim=True) # dim (0, 2, 3) since we wanna collapse the batch dimension (collapsing all rows into a single row), height and width to calculate the mean for all activations in the channels dim.\n",
    "    print(f\"Numerator tensor shape: {numerator.shape}\")\n",
    "    denominator = torch.sqrt(torch.var(x, dim=(0, 2, 3), unbiased=False, keepdim=True) + eps)\n",
    "    normalized_tensor = numerator/denominator\n",
    "    print(f\"Normalized tensor shape: {normalized_tensor.shape}\")\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "image_input_tensor = torch.rand(BATCH_SIZE, NUM_CHANNELS, HEIGHT, WIDTH)\n",
    "normalized_image_tensor = normalize2D(image_input_tensor)\n",
    "normalized_image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b47295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2D(nn.Module):\n",
    "    def __init__(self, n_channels: int, eps: float = 1e-5, momentum: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(self.n_channels))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.n_channels))\n",
    "\n",
    "        # Used to calculate EMA instead of calculating mean and variance of the entire training set after training. \n",
    "        self.register_buffer('running_mean', torch.zeros(n_channels))\n",
    "        self.register_buffer('running_var', torch.ones(n_channels))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x.shape: [B, C, H, W]\n",
    "        if self.training:\n",
    "            batch_mean = torch.mean(x, dim=(0, 2, 3), keepdim=True)\n",
    "            batch_var = torch.var(x, dim=(0, 2, 3), unbiased=False, keepdim=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.squeeze()\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.squeeze()\n",
    "\n",
    "            mean = batch_mean\n",
    "            var = batch_var\n",
    "        else:\n",
    "            mean = self.running_mean.view(1, self.n_features, 1, 1)\n",
    "            var = self.running_var.view(1, self.n_features, 1, 1)\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        scaled_and_shifted = self.gamma.view(1, self.n_channels, 1, 1) * x_hat + self.beta.view(1, self.n_channels, 1, 1)\n",
    "\n",
    "        return scaled_and_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b3f651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5462,  1.6242, -1.3328,  ...,  1.3732,  1.5921,  0.8680],\n",
       "          [-0.1094,  0.3836,  0.6358,  ...,  0.1983, -0.6079, -0.7143],\n",
       "          [ 1.4027,  0.7692, -0.7442,  ...,  0.7702,  1.5141, -0.6277],\n",
       "          ...,\n",
       "          [ 0.6075,  1.6465, -0.5005,  ...,  0.7709, -1.3559,  1.0874],\n",
       "          [ 0.9382, -1.3500,  1.3994,  ..., -1.5106,  1.5770, -0.2618],\n",
       "          [-1.4785,  1.0848,  0.1401,  ...,  0.4633, -0.3789, -0.5244]],\n",
       "\n",
       "         [[ 1.5688, -1.2322, -0.6788,  ..., -1.0756,  0.8480, -0.9172],\n",
       "          [ 0.0134, -0.6316,  1.1542,  ..., -0.6255, -0.1618,  0.0278],\n",
       "          [ 1.1640,  0.2854, -1.5592,  ..., -0.0286, -0.5903,  0.2796],\n",
       "          ...,\n",
       "          [ 1.4502,  1.0023,  1.3632,  ...,  1.7106,  0.2137,  1.4205],\n",
       "          [ 1.4260,  0.6429,  0.5273,  ..., -0.9019, -1.0301,  1.5476],\n",
       "          [-0.4451,  0.6664, -1.1424,  ..., -0.8534, -1.5626,  1.2858]],\n",
       "\n",
       "         [[ 0.0094, -0.3890, -1.0076,  ...,  1.2930, -0.3669, -0.2070],\n",
       "          [-0.6518,  0.5907,  1.1759,  ..., -0.1548,  0.0749, -0.9691],\n",
       "          [ 1.4775, -0.7119, -0.9957,  ...,  0.4541,  0.4218, -0.1627],\n",
       "          ...,\n",
       "          [ 1.3435, -1.5716,  1.6971,  ..., -0.7923, -1.2116, -0.7554],\n",
       "          [-1.2768,  0.6444, -0.3110,  ...,  0.8780, -1.7308, -0.3481],\n",
       "          [-1.5474, -1.3454, -1.3962,  ...,  0.4650, -1.4266,  0.2170]]],\n",
       "\n",
       "\n",
       "        [[[-0.0861,  1.3163, -0.4038,  ..., -0.1161,  0.3632, -0.8677],\n",
       "          [ 0.5173, -0.1843,  1.0478,  ..., -1.6433,  1.6052, -0.1363],\n",
       "          [ 1.5060,  0.6094, -1.5258,  ...,  0.8162, -0.4285,  1.1671],\n",
       "          ...,\n",
       "          [ 0.6916, -1.0775,  1.5971,  ...,  1.2033,  0.2066, -0.6679],\n",
       "          [-1.4154,  1.2864, -0.0718,  ...,  0.5216, -0.5810,  0.3204],\n",
       "          [ 0.2680, -1.6499, -1.6069,  ..., -1.3295,  0.5768, -1.6044]],\n",
       "\n",
       "         [[-1.4025,  0.2178,  1.2720,  ...,  1.4302, -1.5868, -0.0587],\n",
       "          [ 0.8877, -0.7051,  0.5641,  ...,  0.8797, -1.6017,  0.4096],\n",
       "          [ 1.2639,  1.0380, -0.2962,  ..., -0.0999,  0.6634, -1.3866],\n",
       "          ...,\n",
       "          [ 0.2401, -1.5926, -1.3989,  ...,  1.3735,  1.3334, -1.2757],\n",
       "          [-1.4482, -0.6242, -1.1234,  ...,  1.4815,  0.0333, -1.3549],\n",
       "          [-1.6284, -0.4867,  0.4034,  ...,  1.0253, -0.3735, -0.8016]],\n",
       "\n",
       "         [[ 0.6546,  1.6575, -1.5430,  ..., -0.4169,  0.9380, -0.8765],\n",
       "          [-0.0113,  0.7030,  1.3274,  ..., -1.3763, -0.7438,  1.5941],\n",
       "          [ 0.3995, -0.6240,  0.4927,  ...,  1.4510,  0.6818, -0.9771],\n",
       "          ...,\n",
       "          [-1.0438, -0.5381,  0.5102,  ...,  0.2792, -0.3609, -0.0284],\n",
       "          [-1.6408,  0.7045,  0.5920,  ...,  0.3136,  0.0123,  1.5851],\n",
       "          [-0.1008,  0.3946,  0.0411,  ...,  0.1797, -0.4821,  1.5690]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6474, -0.9838,  1.2977,  ...,  1.1344,  0.7300, -0.5064],\n",
       "          [-0.6185, -0.2584,  1.1365,  ...,  1.1996,  0.0633, -0.1296],\n",
       "          [-1.3610,  0.4818,  0.6133,  ..., -0.8230, -0.4814,  0.8903],\n",
       "          ...,\n",
       "          [ 1.4567, -1.4958,  0.2048,  ..., -1.5133,  1.5846, -1.4754],\n",
       "          [-1.1063,  0.0388, -1.4270,  ...,  0.5662,  0.4470, -0.6238],\n",
       "          [ 0.7045, -1.2695,  0.1734,  ...,  0.3214, -0.7873, -1.1664]],\n",
       "\n",
       "         [[-1.0497,  0.1973, -1.6534,  ...,  1.6367,  1.5417,  1.2631],\n",
       "          [-0.6014, -0.4038, -0.2649,  ..., -0.4710,  1.5322, -0.7995],\n",
       "          [ 1.2533, -0.0888,  0.3209,  ...,  0.3610,  0.5377, -0.3934],\n",
       "          ...,\n",
       "          [-0.0728,  0.2163,  1.7291,  ...,  1.6206,  1.6767,  1.5541],\n",
       "          [-1.4314,  0.3138, -1.2907,  ...,  0.7777,  0.0663, -1.0073],\n",
       "          [-0.4274, -0.3943, -0.2392,  ...,  1.0617,  1.3583,  1.1660]],\n",
       "\n",
       "         [[ 1.7225,  0.4618, -1.6884,  ...,  1.4579, -1.2870,  1.0121],\n",
       "          [ 0.3127, -1.3020, -0.8947,  ...,  1.0753, -0.1378,  0.9036],\n",
       "          [-1.3172, -0.1585, -1.3073,  ..., -1.0376,  1.4839,  1.3379],\n",
       "          ...,\n",
       "          [ 0.4616, -0.0301,  0.5170,  ..., -0.6337, -0.7639,  0.0703],\n",
       "          [ 1.3801, -0.7869,  0.5087,  ..., -0.5577, -0.7221,  1.3611],\n",
       "          [ 0.8338,  1.6797,  1.4474,  ..., -1.2981, -0.2063,  0.1611]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.4878, -1.5756, -0.6279,  ..., -0.2567, -1.2981, -1.6725],\n",
       "          [ 0.1729,  0.6638,  0.5383,  ...,  1.2594,  0.1653,  0.1142],\n",
       "          [ 1.2547,  0.4747, -0.6576,  ...,  0.9488, -0.9416, -1.1829],\n",
       "          ...,\n",
       "          [-1.0482,  1.1176,  1.6960,  ...,  0.9667,  0.6550, -0.2982],\n",
       "          [ 1.1449,  0.5750,  1.3133,  ..., -1.1431,  0.8361, -0.0020],\n",
       "          [-1.6847, -1.7362,  1.6439,  ...,  0.6008, -1.2723,  0.5319]],\n",
       "\n",
       "         [[ 0.0060,  0.4085,  0.0519,  ...,  1.2306,  1.3226, -0.4567],\n",
       "          [-1.1878,  1.7076,  0.7836,  ..., -0.7390,  1.3055, -0.1831],\n",
       "          [-0.3232, -0.6725,  0.9597,  ..., -1.6349, -1.4676,  0.1119],\n",
       "          ...,\n",
       "          [ 0.3752, -0.7758,  0.1922,  ...,  0.8355,  0.7941,  0.0425],\n",
       "          [-0.1776,  0.5520,  1.4156,  ..., -0.1746,  0.6549,  1.5671],\n",
       "          [-1.4900, -0.0552,  0.0580,  ...,  1.0776,  0.4744,  0.6743]],\n",
       "\n",
       "         [[ 0.5113, -0.3873, -0.9599,  ..., -0.4262, -0.6771,  1.2349],\n",
       "          [-1.4546,  1.5660, -0.7510,  ...,  0.3563, -0.8951,  1.4700],\n",
       "          [-0.8708, -1.5287, -1.1321,  ..., -1.1055, -1.7053, -0.6186],\n",
       "          ...,\n",
       "          [-1.2849,  0.1354, -0.6458,  ..., -0.7618, -1.0026,  0.6566],\n",
       "          [ 0.6743, -0.3377,  1.4617,  ..., -0.9120,  1.5584, -1.3229],\n",
       "          [-0.5219,  0.9355,  0.1632,  ...,  0.4497,  1.7107, -0.6337]]],\n",
       "\n",
       "\n",
       "        [[[-0.7176, -1.0587,  0.6049,  ..., -0.9959, -1.4069,  1.5939],\n",
       "          [ 1.4860, -1.6403,  1.0132,  ..., -0.8695, -0.0115,  1.2371],\n",
       "          [-0.5051,  0.0913, -1.4735,  ..., -0.1879, -1.4417, -0.0734],\n",
       "          ...,\n",
       "          [-0.5891,  1.6064, -1.7017,  ...,  1.2889, -0.4256, -0.5216],\n",
       "          [-0.6655,  0.0598, -0.8442,  ..., -1.0653, -1.7293,  0.0666],\n",
       "          [ 0.8607,  0.0210,  0.9065,  ..., -0.8436,  1.3870, -1.0306]],\n",
       "\n",
       "         [[ 1.2841, -1.2699, -1.4738,  ..., -0.1535, -0.4343,  0.7852],\n",
       "          [ 0.2501, -1.7045,  0.0755,  ...,  1.5453, -0.8101, -0.3407],\n",
       "          [ 0.2298,  0.0153,  0.7126,  ...,  1.1968,  0.6302,  0.4988],\n",
       "          ...,\n",
       "          [-0.6593,  1.0051,  1.6644,  ...,  1.3544, -1.3328,  0.2397],\n",
       "          [ 0.4554, -0.4132, -0.0375,  ..., -1.5392, -1.1037, -0.2395],\n",
       "          [ 0.0018,  0.1165,  1.1747,  ...,  1.4481,  0.8743,  0.3066]],\n",
       "\n",
       "         [[ 0.8470,  1.1814,  0.1266,  ...,  0.3507,  1.0169, -0.7480],\n",
       "          [-1.6717, -1.6314,  0.4497,  ..., -0.5877, -0.2700,  0.1487],\n",
       "          [ 0.7209,  1.5694,  0.4510,  ...,  0.1497, -0.3699, -1.7324],\n",
       "          ...,\n",
       "          [-1.0375,  0.4266, -0.2556,  ...,  0.0953,  1.1038,  1.1071],\n",
       "          [-0.9310, -0.6391, -1.3580,  ...,  0.0465, -1.3884,  0.5642],\n",
       "          [ 0.6369,  1.4604,  1.5862,  ...,  1.2905, -1.3803, -1.4250]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1783,  0.9313,  0.9626,  ...,  1.4065, -1.0496, -1.4702],\n",
       "          [-0.3371, -1.5358,  0.6316,  ...,  1.4387, -1.0042,  1.2947],\n",
       "          [ 0.2516, -0.7584, -0.4028,  ...,  0.4719,  1.3419,  0.8768],\n",
       "          ...,\n",
       "          [-1.4730, -0.6986, -1.6735,  ..., -1.4254, -0.6669, -0.1390],\n",
       "          [-0.7115,  1.1086, -0.8492,  ..., -1.5107, -1.2245, -1.2545],\n",
       "          [ 1.5474, -0.1587,  1.1908,  ..., -1.0158, -1.1143, -0.6516]],\n",
       "\n",
       "         [[-0.4145, -1.0606,  0.2284,  ..., -1.1946,  1.6192, -0.2383],\n",
       "          [ 1.3136, -1.3552, -1.1382,  ..., -0.6078, -1.3073, -0.4038],\n",
       "          [ 1.5168,  0.0556, -1.6609,  ...,  0.4456,  0.2563, -1.4776],\n",
       "          ...,\n",
       "          [ 1.5110,  0.8137,  1.2720,  ...,  1.4149,  0.6939,  0.1999],\n",
       "          [ 0.8285,  0.2125,  0.5423,  ...,  0.7263,  0.5505,  0.5500],\n",
       "          [ 0.5525,  0.2149,  0.6048,  ...,  0.0211, -0.4632,  0.2502]],\n",
       "\n",
       "         [[ 1.1705,  0.6101, -1.6365,  ...,  1.2521,  0.9974, -0.4763],\n",
       "          [ 0.1291, -1.0920,  0.3834,  ..., -0.9257, -0.9869,  1.6382],\n",
       "          [-1.2833, -0.2416,  1.1349,  ...,  1.6960,  0.2186, -1.0651],\n",
       "          ...,\n",
       "          [ 0.7712,  1.5490,  1.5444,  ...,  0.4325, -1.0983, -1.5370],\n",
       "          [-0.7538, -0.8353, -0.4484,  ...,  1.3922, -0.3739, -0.2480],\n",
       "          [-1.0615, -1.5226,  0.7351,  ..., -0.4093,  0.7675, -1.6726]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm_2d = BatchNorm2D(n_channels=3)\n",
    "normalized_image_tensor2 = batchnorm_2d(image_input_tensor)\n",
    "normalized_image_tensor2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_base_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
